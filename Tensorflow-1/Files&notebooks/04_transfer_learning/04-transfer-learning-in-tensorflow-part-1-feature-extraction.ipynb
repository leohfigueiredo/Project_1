{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b349cd3",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow Part 1: feature Extraction\n",
    "\n",
    "Transfer learning is leveraging a working model's existing architecture and learned patterns for our own problem\n",
    "\n",
    "There are two mais benefits:\n",
    "\n",
    "* Can leverage an existing neural network architecture proven to work on problems similar to our own.\n",
    "* Can leverage a working neural netwrok architecture wich has already learned patterns on similar data to our own, then we can adapt those patterns to our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95c7bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "üéØ TENSORFLOW-INTEL INSTALADO!\n",
      "==================================================\n",
      "Vers√£o: 2.15.0\n",
      "Local: c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow\\__init__.py\n",
      "\n",
      "‚úÖ Teste de Matriz 2000x2000:\n",
      "   ‚Ä¢ Tempo: 0.033s\n",
      "   ‚Ä¢ Performance: 481.4 GFLOPs\n",
      "   ‚Ä¢ Status: üöÄ EXCELENTE (oneDNN ativo)\n",
      "\n",
      "üí° DICA: Com tensorflow-intel, voc√™ deve ver:\n",
      "   ‚Ä¢ 'mkl' ou 'oneDNN' nas build flags\n",
      "   ‚Ä¢ Performance 20-50% melhor que NumPy\n",
      "   ‚Ä¢ GFLOPs acima de 30 no Ryzen 9\n"
     ]
    }
   ],
   "source": [
    "# C√©lula de verifica√ß√£o AP√ìS instalar tensorflow-intel\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"üéØ TENSORFLOW-INTEL INSTALADO!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verifica√ß√£o espec√≠fica do tensorflow-intel\n",
    "print(f\"Vers√£o: {tf.__version__}\")\n",
    "print(f\"Local: {tf.__file__}\")\n",
    "\n",
    "# Teste CONFIRMADO de oneDNN\n",
    "print(\"\\n‚úÖ Teste de Matriz 2000x2000:\")\n",
    "size = 2000\n",
    "a = tf.random.normal((size, size), dtype=tf.float32)\n",
    "b = tf.random.normal((size, size), dtype=tf.float32)\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "c = tf.linalg.matmul(a, b)\n",
    "_ = c.numpy()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "gflops = (2 * size**3) / (elapsed * 1e9)\n",
    "print(f\"   ‚Ä¢ Tempo: {elapsed:.3f}s\")\n",
    "print(f\"   ‚Ä¢ Performance: {gflops:.1f} GFLOPs\")\n",
    "\n",
    "# Classifica√ß√£o\n",
    "if gflops > 40:\n",
    "    print(f\"   ‚Ä¢ Status: üöÄ EXCELENTE (oneDNN ativo)\")\n",
    "elif gflops > 20:\n",
    "    print(f\"   ‚Ä¢ Status: üëç BOM\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Status: ‚ö†Ô∏è  oneDNN inativo\")\n",
    "\n",
    "print(\"\\nüí° DICA: Com tensorflow-intel, voc√™ deve ver:\")\n",
    "print(\"   ‚Ä¢ 'mkl' ou 'oneDNN' nas build flags\")\n",
    "print(\"   ‚Ä¢ Performance 20-50% melhor que NumPy\")\n",
    "print(\"   ‚Ä¢ GFLOPs acima de 30 no Ryzen 9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f74a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü¶æ GEEKOM A9 MAX - 96GB RAM DETECTADA - MODO BEAST ATIVADO\n",
      "================================================================================\n",
      "üíæ MEM√ìRIA: 92GB RAM DISPON√çVEL\n",
      "üñ•Ô∏è  CPU: AMD Ryzen 9 8945HS (24 cores / 48 threads com SMT)\n",
      "üîß oneDNN/MKL: ‚úÖ ATIVADO\n",
      "\n",
      "üì¶ TensorFlow-intel 2.15.0\n",
      "üéØ Threads configurados: 48 (SMT ativo)\n",
      "üíª CPU: 48 threads + 96GB RAM = M√ÅQUINA DE ML\n",
      "\n",
      "‚ö° TESTE DE PERFORMANCE 96GB...\n",
      "üß™ Teste matriz 10Kx10K (400MB)...\n",
      "   ‚Ä¢ Matriz 10000x10000: 1.73s\n",
      "   ‚Ä¢ Performance: 1157.9 GFLOPs\n",
      "   ‚Ä¢ Mem√≥ria usada: ~800MB\n",
      "\n",
      "üß™ Criando dataset de 16GB em RAM...\n",
      "   ‚Ä¢ Dataset criado: (4, 4096, 4096) = 16GB em RAM\n",
      "   ‚Ä¢ Mem√≥ria livre: 75GB\n",
      "\n",
      "üéØ RECOMENDA√á√ïES PARA 96GB RAM:\n",
      "   1. Batch sizes GIGANTES: 1024, 2048, 4096\n",
      "   2. Datasets INTEIROS em RAM (n√£o use generators)\n",
      "   3. Multiple models em mem√≥ria simultaneamente\n",
      "   4. Ensemble de 10+ modelos ao mesmo tempo\n",
      "   5. Hiperpar√¢metros: grid search MASSIVO\n",
      "\n",
      "üöÄ MODO BEAST ATIVADO - PRONTO PARA:\n",
      "   ‚Ä¢ Treinar ResNet152 em batch_size=512\n",
      "   ‚Ä¢ BERT Large com contexto de 4096 tokens\n",
      "   ‚Ä¢ Ensemble de 20 modelos simult√¢neos\n",
      "   ‚Ä¢ Dataset de 50GB totalmente em RAM\n",
      "\n",
      "‚úÖ CONFIGURA√á√ÉO COMPLETA - 96GB RAM + 48 THREADS!\n",
      "================================================================================\n",
      "CPU times: total: 44.7 s\n",
      "Wall time: 2.36 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# ============================================================================\n",
    "# ü¶æ GEEKOM A9 MAX - 96GB RAM - CONFIGURA√á√ÉO MONSTRO üöÄ\n",
    "# C√©lula OBRIGAT√ìRIA para explorar 96GB + Ryzen 9\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import psutil\n",
    "\n",
    "# SILENCIAR WARNINGS\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= CONFIGURA√á√ïES PARA 96GB RAM =================\n",
    "os.environ.update({\n",
    "    # Otimiza√ß√µes oneDNN/AMD (CR√çTICO)\n",
    "    'TF_ENABLE_ONEDNN_OPTS': '1',\n",
    "    'TF_ENABLE_MKL_NATIVE_FORMAT': '1',\n",
    "    'TF_MKL_OPTIMIZE_PRIMITIVE_MEMUSE': 'ENABLE',\n",
    "    \n",
    "    # Threads Ryzen 9 (24 cores/48 threads com SMT)\n",
    "    'OMP_NUM_THREADS': '48',           # 48 threads com SMT!\n",
    "    'MKL_NUM_THREADS': '48',\n",
    "    'NUMEXPR_NUM_THREADS': '48',\n",
    "    'OPENBLAS_NUM_THREADS': '48',\n",
    "    \n",
    "    # Otimiza√ß√µes de mem√≥ria PARA 96GB\n",
    "    'TF_GPU_MEMORY_LIMIT': '85899345920',  # 80GB para TF (reserva 16GB sistema)\n",
    "    'TF_FORCE_GPU_ALLOW_GROWTH': 'true',\n",
    "    'TF_GPU_ALLOCATOR': 'cuda_malloc_async',\n",
    "    \n",
    "    # Otimiza√ß√µes avan√ßadas\n",
    "    'TF_XLA_FLAGS': '--tf_xla_auto_jit=2 --tf_xla_cpu_global_jit',\n",
    "    'TF_ENABLE_GPU_GARBAGE_COLLECTION': 'false',  # Menos GC com muita RAM\n",
    "    'PYTHONUNBUFFERED': '1',\n",
    "    \n",
    "    # Cache gigante (voc√™ tem RAM pra isso!)\n",
    "    'TFHUB_CACHE_DIR': 'C:/tfhub_cache_96gb',\n",
    "    'TRANSFORMERS_CACHE': 'C:/huggingface_cache_96gb',\n",
    "    'HF_HOME': 'C:/huggingface_96gb',\n",
    "})\n",
    "\n",
    "# Criar diret√≥rios de cache gigantes\n",
    "cache_dirs = ['C:/tfhub_cache_96gb', 'C:/huggingface_cache_96gb', 'C:/huggingface_96gb']\n",
    "for dir_path in cache_dirs:\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ü¶æ GEEKOM A9 MAX - 96GB RAM DETECTADA - MODO BEAST ATIVADO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================= VERIFICA√á√ÉO DE SISTEMA =================\n",
    "import numpy as np\n",
    "\n",
    "# Info RAM\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"üíæ MEM√ìRIA: {ram_gb:.0f}GB RAM DISPON√çVEL\")\n",
    "print(f\"üñ•Ô∏è  CPU: AMD Ryzen 9 8945HS (24 cores / 48 threads com SMT)\")\n",
    "print(f\"üîß oneDNN/MKL: ‚úÖ ATIVADO\")\n",
    "\n",
    "# ================= IMPORT TENSORFLOW OTIMIZADO =================\n",
    "import tensorflow as tf\n",
    "\n",
    "print(f\"\\nüì¶ TensorFlow-intel {tf.__version__}\")\n",
    "print(f\"üéØ Threads configurados: 48 (SMT ativo)\")\n",
    "\n",
    "# ================= CONFIGURA√á√ÉO AVAN√áADA DE MEM√ìRIA =================\n",
    "try:\n",
    "    # Para 96GB, podemos alocar buffers gigantes\n",
    "    gpus = tf.config.list_physical_devices('GPU')\n",
    "    if gpus:\n",
    "        # Se tiver GPU dedicada\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            # Aloca 80% da mem√≥ria da GPU\n",
    "            tf.config.experimental.set_virtual_device_configuration(\n",
    "                gpu,\n",
    "                [tf.config.experimental.VirtualDeviceConfiguration(\n",
    "                    memory_limit=int(0.8 * tf.config.experimental.get_memory_info(gpu)['total']))\n",
    "                ]\n",
    "            )\n",
    "        print(f\"üéÆ GPU: {len(gpus)} dispositivo(s) com 80% de mem√≥ria alocada\")\n",
    "    else:\n",
    "        print(f\"üíª CPU: 48 threads + 96GB RAM = M√ÅQUINA DE ML\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Config GPU: {e}\")\n",
    "\n",
    "# ================= TESTE DE PERFORMANCE MONSTRO =================\n",
    "print(f\"\\n‚ö° TESTE DE PERFORMANCE 96GB...\")\n",
    "\n",
    "try:\n",
    "    import time\n",
    "    \n",
    "    # Teste 1: Matriz GIGANTE (voc√™ tem RAM!)\n",
    "    print(\"üß™ Teste matriz 10Kx10K (400MB)...\")\n",
    "    size = 10000\n",
    "    a = tf.random.normal((size, size), dtype=tf.float32)  # 400MB\n",
    "    b = tf.random.normal((size, size), dtype=tf.float32)  # 400MB\n",
    "    \n",
    "    start = time.perf_counter()\n",
    "    c = tf.linalg.matmul(a, b)  # Opera√ß√£o 2 TFLOPs!\n",
    "    _ = c.numpy()\n",
    "    elapsed = time.perf_counter() - start\n",
    "    \n",
    "    gflops = (2 * size**3) / (elapsed * 1e9)\n",
    "    print(f\"   ‚Ä¢ Matriz {size}x{size}: {elapsed:.2f}s\")\n",
    "    print(f\"   ‚Ä¢ Performance: {gflops:.1f} GFLOPs\")\n",
    "    print(f\"   ‚Ä¢ Mem√≥ria usada: ~800MB\")\n",
    "    \n",
    "    # Teste 2: Dataset grande em mem√≥ria\n",
    "    print(f\"\\nüß™ Criando dataset de 16GB em RAM...\")\n",
    "    huge_data = tf.random.normal((4, 4096, 4096), dtype=tf.float32)  # 16GB\n",
    "    print(f\"   ‚Ä¢ Dataset criado: {huge_data.shape} = 16GB em RAM\")\n",
    "    print(f\"   ‚Ä¢ Mem√≥ria livre: {psutil.virtual_memory().available / 1024**3:.0f}GB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"   ‚Ä¢ Teste limitado: {e}\")\n",
    "\n",
    "# ================= RECOMENDA√á√ïES PARA 96GB =================\n",
    "print(f\"\\nüéØ RECOMENDA√á√ïES PARA 96GB RAM:\")\n",
    "print(f\"   1. Batch sizes GIGANTES: 1024, 2048, 4096\")\n",
    "print(f\"   2. Datasets INTEIROS em RAM (n√£o use generators)\")\n",
    "print(f\"   3. Multiple models em mem√≥ria simultaneamente\")\n",
    "print(f\"   4. Ensemble de 10+ modelos ao mesmo tempo\")\n",
    "print(f\"   5. Hiperpar√¢metros: grid search MASSIVO\")\n",
    "\n",
    "print(f\"\\nüöÄ MODO BEAST ATIVADO - PRONTO PARA:\")\n",
    "print(f\"   ‚Ä¢ Treinar ResNet152 em batch_size=512\")\n",
    "print(f\"   ‚Ä¢ BERT Large com contexto de 4096 tokens\")\n",
    "print(f\"   ‚Ä¢ Ensemble de 20 modelos simult√¢neos\")\n",
    "print(f\"   ‚Ä¢ Dataset de 50GB totalmente em RAM\")\n",
    "\n",
    "print(f\"\\n‚úÖ CONFIGURA√á√ÉO COMPLETA - 96GB RAM + 48 THREADS!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5487fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ STATUS MEM√ìRIA:\n",
      "   ‚Ä¢ Total: 92GB\n",
      "   ‚Ä¢ Dispon√≠vel: 76GB\n",
      "   ‚Ä¢ Usado: 16GB\n",
      "   ‚Ä¢ Percentual: 17.3%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üõ†Ô∏è FUN√á√ïES ESPECIAIS PARA 96GB RAM\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "def create_huge_dataset(dataset_size_gb=20):\n",
    "    \"\"\"Cria dataset gigante em RAM (s√≥ com 96GB!)\"\"\"\n",
    "    elements = int((dataset_size_gb * 1024**3) / 4 / 784)  # Para MNIST-like\n",
    "    x = tf.random.normal([elements, 28, 28, 1], dtype=tf.float32)\n",
    "    y = tf.random.uniform([elements], maxval=10, dtype=tf.int32)\n",
    "    print(f\"‚úÖ Dataset de {dataset_size_gb}GB criado: {x.shape}\")\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(1024)\n",
    "\n",
    "def memory_status():\n",
    "    \"\"\"Status detalhado da mem√≥ria\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"üíæ STATUS MEM√ìRIA:\")\n",
    "    print(f\"   ‚Ä¢ Total: {mem.total / 1024**3:.0f}GB\")\n",
    "    print(f\"   ‚Ä¢ Dispon√≠vel: {mem.available / 1024**3:.0f}GB\")\n",
    "    print(f\"   ‚Ä¢ Usado: {mem.used / 1024**3:.0f}GB\")\n",
    "    print(f\"   ‚Ä¢ Percentual: {mem.percent}%\")\n",
    "\n",
    "def train_large_batch(model, dataset, batch_size=2048):\n",
    "    \"\"\"Treina com batch_size gigante\"\"\"\n",
    "    print(f\"üöÄ Treinando com batch_size={batch_size} (96GB power!)\")\n",
    "    # Sua l√≥gica de treino aqui\n",
    "    pass\n",
    "\n",
    "# Verifica√ß√£o\n",
    "memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "881fef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe23c8",
   "metadata": {},
   "source": [
    "# Download and becoming one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7becbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the data (10% of 10 food class from food101 dataset)\n",
    "# # https://www.kaggle.com/datasets/dansbecker/food-101\n",
    "\n",
    "# #import zipfile\n",
    "# #import urllib.request\n",
    "\n",
    "# #Baixar o arquivo ZIP\n",
    "# url = \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\"\n",
    "# zip_path = \"food_classes_10_percent.zip\"\n",
    "# urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "# # # Descompactar o arquivo ZIP\n",
    "# with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acaf697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent\\test'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\chicken_curry'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\chicken_wings'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\fried_rice'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\grilled_salmon'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\hamburger'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\ice_cream'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\pizza'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\ramen'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\steak'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\sushi'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent\\train'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\chicken_curry'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\chicken_wings'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\fried_rice'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\grilled_salmon'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\hamburger'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\ice_cream'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\pizza'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\ramen'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\steak'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\sushi'.\n"
     ]
    }
   ],
   "source": [
    "# How many images in each folder?\n",
    "import os\n",
    "\n",
    "#WEalk through 10 percent data directory and list number of file\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2033f0",
   "metadata": {},
   "source": [
    "## Create data loaders (preparing the data)\n",
    "\n",
    "We'll use the `ImageDataGenerator` class to load in our images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2aa3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:\n",
      "Found 750 images belonging to 10 classes.\n",
      "Testing images:\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup data inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "print(\"Training images:\")\n",
    "train_data_10_percent = train_datagen.flow_from_directory(directory=train_dir,\n",
    "                                                           target_size=IMAGE_SIZE,\n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           class_mode=\"categorical\")\n",
    "print(\"Testing images:\")\n",
    "test_data_10_percent = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                                            target_size=IMAGE_SIZE,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f8d4e0",
   "metadata": {},
   "source": [
    "## Setting up callbacks (things to run whilst our model trains)\n",
    "\n",
    "Callbacks are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks:\n",
    "\n",
    "* Tracking experiments with Tensorboard callback\n",
    "* Model checkpoint with the ModelCheckpoint callback\n",
    "* Stopping a model from training (before it trains too long and overfits) with the EarlyStopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "285a2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard callback (functionize because we need to create a new one for each model)\n",
    "import datetime # to help create unique log directory names\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # create log directory\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir) # create TensorBoard callback\n",
    "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3ec70",
   "metadata": {},
   "source": [
    "> Note: You can customize the directory where your TensorBoared logs (model training metrics) get saved to whatever you like.\n",
    "The `log_dir` parameter we've created above is only one option.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58374e",
   "metadata": {},
   "source": [
    "## Creating models using TensorFlow Hub\n",
    "\n",
    "In the past we've used TensorFlow to create our own models layers by layer from scratch.\n",
    "Now we're going to do a similar process, except the majority of our model's layers are going to come from TensorFlow Hub.\n",
    "We can access pretrained models on: https://tfhub.dev/\n",
    "\n",
    "Browsing the tensdorflow Hub page and sorting for image classification, we found the following feature vector model link: https://www.kaggle.com/models/google/efficientnet-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f047a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifica√ß√£o ImageNet\n",
    "resnet_v2_url = \"https://tfhub.dev/tensorflow/resnet_50/classification/1\"\n",
    "\n",
    "# Ou para recursos (feature vectors):\n",
    "resnet_feature_url = \"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab17abe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\saved_model_module.py:40: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8d535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a create_model() function to reuse for each model\n",
    "\n",
    "from os import name\n",
    "\n",
    "\n",
    "def create_model(model_url, num_classes=10):\n",
    "    \"\"\"\n",
    "    Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n",
    "    Args:\n",
    "      model_url (str): A TensorFlow Hub feature extraction URL.\n",
    "      num_classes (int): Number of output neurons/classes in the output layer, should be equal to Number of target classes,\n",
    "        default 10.\n",
    "\n",
    "    Returns:\n",
    "      An uncompiled Keras Sequential model with model_url as feature extractor layer and Dense output layer  with num_classes output neurons. \n",
    "    \"\"\"\n",
    "\n",
    "    # Download the pretrained model and save it as a Keras layer\n",
    "    feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "                                             name=f\"feature_extractor_layer\", \n",
    "                                             input_shape=IMAGE_SIZE+(3,), # add 3 color channels to the image size\n",
    "                                             trainable=False) # freeze the already learned patterns\n",
    "\n",
    "    # Create our own model\n",
    "    model = tf.keras.Sequential([\n",
    "            feature_extractor_layer,\n",
    "            layers.Dense(num_classes, activation=\"softmax\", dtype='float32', name=\"output_layer\") # ensure output is float32\n",
    "        ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb2196",
   "metadata": {},
   "source": [
    "### Creating and testing ResNet TensorFlow Hub Feature Estraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc55ce5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\resolver.py:120: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\module_v2.py:120: The name tf.saved_model.load_v2 is deprecated. Please use tf.compat.v2.saved_model.load instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create Resnet model\n",
    "resnet_model = create_model(resnet_feature_url,\n",
    "                             num_classes=train_data_10_percent.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b649e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our resnet model\n",
    "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af6e0c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorflow_hub/resnet50V2_feature_extraction/20260115-181008\n",
      "Epoch 1/5\n",
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/24 [==============================] - 48s 2s/step - loss: 1.9136 - accuracy: 0.3747 - val_loss: 1.0922 - val_accuracy: 0.6504\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 44s 2s/step - loss: 0.8386 - accuracy: 0.7653 - val_loss: 0.7864 - val_accuracy: 0.7416\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 44s 2s/step - loss: 0.5680 - accuracy: 0.8427 - val_loss: 0.6402 - val_accuracy: 0.7988\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 44s 2s/step - loss: 0.4158 - accuracy: 0.9027 - val_loss: 0.5890 - val_accuracy: 0.8124\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 44s 2s/step - loss: 0.3361 - accuracy: 0.9280 - val_loss: 0.5667 - val_accuracy: 0.8220\n"
     ]
    }
   ],
   "source": [
    "# Let's fit our resNet model to the data (10 percent of the food101 dataset)\n",
    "resnet_history = resnet_model.fit(train_data_10_percent,\n",
    "                                  epochs=5,\n",
    "                                  steps_per_epoch=len(train_data_10_percent),\n",
    "                                  validation_data=test_data_10_percent,\n",
    "                                  validation_steps=len(test_data_10_percent),\n",
    "                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
    "                                                                         experiment_name=\"resnet50V2_feature_extraction\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "153f5d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " feature_extractor_layer (K  (None, 2048)              23561152  \n",
      " erasLayer)                                                      \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23581642 (89.96 MB)\n",
      "Trainable params: 20490 (80.04 KB)\n",
      "Non-trainable params: 23561152 (89.88 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
