{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b349cd3",
   "metadata": {},
   "source": [
    "# Transfer Learning with TensorFlow Part 1: feature Extraction\n",
    "\n",
    "Transfer learning is leveraging a working model's existing architecture and learned patterns for our own problem\n",
    "\n",
    "There are two mais benefits:\n",
    "\n",
    "* Can leverage an existing neural network architecture proven to work on problems similar to our own.\n",
    "* Can leverage a working neural netwrok architecture wich has already learned patterns on similar data to our own, then we can adapt those patterns to our own data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "95c7bd4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ TENSORFLOW-INTEL INSTALADO!\n",
      "==================================================\n",
      "Vers√£o: 2.15.0\n",
      "Local: c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow\\__init__.py\n",
      "\n",
      "‚úÖ Teste de Matriz 2000x2000:\n",
      "   ‚Ä¢ Tempo: 0.020s\n",
      "   ‚Ä¢ Performance: 782.1 GFLOPs\n",
      "   ‚Ä¢ Status: üöÄ EXCELENTE (oneDNN ativo)\n",
      "\n",
      "üí° DICA: Com tensorflow-intel, voc√™ deve ver:\n",
      "   ‚Ä¢ 'mkl' ou 'oneDNN' nas build flags\n",
      "   ‚Ä¢ Performance 20-50% melhor que NumPy\n",
      "   ‚Ä¢ GFLOPs acima de 30 no Ryzen 9\n"
     ]
    }
   ],
   "source": [
    "# C√©lula de verifica√ß√£o AP√ìS instalar tensorflow-intel\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '1'\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '1'\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "print(\"üéØ TENSORFLOW-INTEL INSTALADO!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Verifica√ß√£o espec√≠fica do tensorflow-intel\n",
    "print(f\"Vers√£o: {tf.__version__}\")\n",
    "print(f\"Local: {tf.__file__}\")\n",
    "\n",
    "# Teste CONFIRMADO de oneDNN\n",
    "print(\"\\n‚úÖ Teste de Matriz 2000x2000:\")\n",
    "size = 2000\n",
    "a = tf.random.normal((size, size), dtype=tf.float32)\n",
    "b = tf.random.normal((size, size), dtype=tf.float32)\n",
    "\n",
    "import time\n",
    "start = time.perf_counter()\n",
    "c = tf.linalg.matmul(a, b)\n",
    "_ = c.numpy()\n",
    "elapsed = time.perf_counter() - start\n",
    "\n",
    "gflops = (2 * size**3) / (elapsed * 1e9)\n",
    "print(f\"   ‚Ä¢ Tempo: {elapsed:.3f}s\")\n",
    "print(f\"   ‚Ä¢ Performance: {gflops:.1f} GFLOPs\")\n",
    "\n",
    "# Classifica√ß√£o\n",
    "if gflops > 40:\n",
    "    print(f\"   ‚Ä¢ Status: üöÄ EXCELENTE (oneDNN ativo)\")\n",
    "elif gflops > 20:\n",
    "    print(f\"   ‚Ä¢ Status: üëç BOM\")\n",
    "else:\n",
    "    print(f\"   ‚Ä¢ Status: ‚ö†Ô∏è  oneDNN inativo\")\n",
    "\n",
    "print(\"\\nüí° DICA: Com tensorflow-intel, voc√™ deve ver:\")\n",
    "print(\"   ‚Ä¢ 'mkl' ou 'oneDNN' nas build flags\")\n",
    "print(\"   ‚Ä¢ Performance 20-50% melhor que NumPy\")\n",
    "print(\"   ‚Ä¢ GFLOPs acima de 30 no Ryzen 9\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5f74a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ü¶æ GEEKOM A9 MAX - CONFIGURA√á√ÉO SEGURA INICIADA\n",
      "================================================================================\n",
      "üíæ MEM√ìRIA: 92GB RAM DISPON√çVEL\n",
      "üñ•Ô∏è  CPU: AMD Ryzen 9 8945HS (24 cores)\n",
      "üîß Configura√ß√£o: 16 threads inicial (seguro)\n",
      "\n",
      "üì¶ TensorFlow-intel 2.15.0 carregado com sucesso\n",
      "üéØ Threads configurados: 16\n",
      "üîß oneDNN: ‚úÖ ATIVADO\n",
      "\n",
      "üß™ Teste leve de funcionamento...\n",
      "‚úÖ Opera√ß√£o b√°sica OK: [[19. 22.]\n",
      " [43. 50.]]\n",
      "\n",
      "‚ö° AGORA voc√™ pode usar toda a pot√™ncia:\n",
      "   1. Aumentar threads gradualmente\n",
      "   2. Criar datasets gigantes\n",
      "   3. Usar batch_size grande\n",
      "\n",
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "üéØ COMO USAR ESTE AMBIENTE:\n",
      "üéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØüéØ\n",
      "1. Esta c√©lula j√° carregou TensorFlow com configura√ß√£o SEGURA\n",
      "2. Para ATIVAR MODO 96GB COMPLETO, execute:\n",
      "   ativar_modo_beast()\n",
      "3. Para TESTAR PERFORMANCE, execute:\n",
      "   teste_performance_96gb(5000)  # 5Kx5K matriz\n",
      "4. Agora importe outras bibliotecas normalmente\n",
      "\n",
      "‚úÖ AMBIENTE INICIALIZADO COM SUCESSO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# ü¶æ GEEKOM A9 MAX - 96GB RAM - CONFIGURA√á√ÉO SEGURA üöÄ\n",
    "# C√©lula INICIAL segura (sem %%time que crasha kernel)\n",
    "# ============================================================================\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "# ================= CONFIGURA√á√ÉO ANTI-CRASH =================\n",
    "# DESATIVE VALIDA√á√ïES QUE CAUSAM KERNEL DEATH\n",
    "os.environ['PYDEVD_DISABLE_FILE_VALIDATION'] = '1'  # MAIS IMPORTANTE!\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'           # S√≥ erros\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ================= CONFIGURA√á√ÉO SEGURA PARA 96GB =================\n",
    "# REDUZA THREADS INICIALMENTE para evitar overcommit\n",
    "os.environ.update({\n",
    "    # Otimiza√ß√µes oneDNN/AMD (mas seguras)\n",
    "    'TF_ENABLE_ONEDNN_OPTS': '1',\n",
    "    \n",
    "    # THREADS REDUZIDOS inicialmente (evita crash)\n",
    "    'OMP_NUM_THREADS': '16',           # 16 threads (n√£o 48!)\n",
    "    'MKL_NUM_THREADS': '8',            # 8 threads MKL\n",
    "    'NUMEXPR_NUM_THREADS': '8',\n",
    "    \n",
    "    # Otimiza√ß√µes seguras de mem√≥ria\n",
    "    'TF_FORCE_GPU_ALLOW_GROWTH': 'true',\n",
    "    'PYTHONUNBUFFERED': '1',\n",
    "    \n",
    "    # Cache diret√≥rios (criar depois)\n",
    "    'TFHUB_CACHE_DIR': 'C:/tfhub_cache',\n",
    "    'TRANSFORMERS_CACHE': 'C:/huggingface_cache',\n",
    "})\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ü¶æ GEEKOM A9 MAX - CONFIGURA√á√ÉO SEGURA INICIADA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================= IMPORTS B√ÅSICOS PRIMEIRO =================\n",
    "import psutil\n",
    "import numpy as np\n",
    "\n",
    "# Info RAM\n",
    "ram_gb = psutil.virtual_memory().total / (1024**3)\n",
    "print(f\"üíæ MEM√ìRIA: {ram_gb:.0f}GB RAM DISPON√çVEL\")\n",
    "print(f\"üñ•Ô∏è  CPU: AMD Ryzen 9 8945HS (24 cores)\")\n",
    "print(f\"üîß Configura√ß√£o: 16 threads inicial (seguro)\")\n",
    "\n",
    "# ================= IMPORT TENSORFLOW SEGURO =================\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    print(f\"\\nüì¶ TensorFlow-intel {tf.__version__} carregado com sucesso\")\n",
    "    \n",
    "    # Verifica√ß√£o b√°sica\n",
    "    print(f\"üéØ Threads configurados: {os.environ.get('OMP_NUM_THREADS')}\")\n",
    "    print(f\"üîß oneDNN: {'‚úÖ ATIVADO' if os.environ.get('TF_ENABLE_ONEDNN_OPTS') == '1' else '‚ùå'}\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"\\n‚ùå ERRO ao importar TensorFlow: {e}\")\n",
    "    print(\"üí° Execute: pip install tensorflow-intel==2.15.0\")\n",
    "    tf = None\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ö†Ô∏è  Aviso TensorFlow: {e}\")\n",
    "    tf = None\n",
    "\n",
    "# ================= TESTE LEVE (OPCIONAL) =================\n",
    "if tf is not None:\n",
    "    print(f\"\\nüß™ Teste leve de funcionamento...\")\n",
    "    try:\n",
    "        # Teste MUITO leve para n√£o crashar\n",
    "        size = 100  # Pequeno!\n",
    "        a = tf.constant([[1.0, 2.0], [3.0, 4.0]])\n",
    "        b = tf.constant([[5.0, 6.0], [7.0, 8.0]])\n",
    "        c = tf.matmul(a, b)\n",
    "        print(f\"‚úÖ Opera√ß√£o b√°sica OK: {c.numpy()}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Teste b√°sico falhou: {e}\")\n",
    "\n",
    "# ================= ATUALIZA√á√ÉO PARA MODO 96GB (AP√ìS IMPORTS) =================\n",
    "print(f\"\\n‚ö° AGORA voc√™ pode usar toda a pot√™ncia:\")\n",
    "print(f\"   1. Aumentar threads gradualmente\")\n",
    "print(f\"   2. Criar datasets gigantes\")\n",
    "print(f\"   3. Usar batch_size grande\")\n",
    "\n",
    "# ================= FUN√á√ÉO PARA ATIVAR MODO BEAST (usar depois) =================\n",
    "def ativar_modo_beast():\n",
    "    \"\"\"Ativa configura√ß√£o m√°xima 96GB - USAR DEPOIS que kernel est√° est√°vel\"\"\"\n",
    "    print(\"\\n\" + \"üî•\" * 40)\n",
    "    print(\"üî• ATIVANDO MODO BEAST 96GB!\")\n",
    "    print(\"üî•\" * 40)\n",
    "    \n",
    "    # Aumenta threads para m√°ximo\n",
    "    os.environ.update({\n",
    "        'OMP_NUM_THREADS': '48',\n",
    "        'MKL_NUM_THREADS': '48',\n",
    "        'NUMEXPR_NUM_THREADS': '48',\n",
    "        'OPENBLAS_NUM_THREADS': '48',\n",
    "    })\n",
    "    \n",
    "    # Configura√ß√µes avan√ßadas\n",
    "    os.environ['TF_XLA_FLAGS'] = '--tf_xla_auto_jit=2'\n",
    "    os.environ['TF_GPU_MEMORY_LIMIT'] = str(80 * 1024**3)  # 80GB\n",
    "    \n",
    "    # Criar cache gigante\n",
    "    cache_dirs = ['C:/tfhub_cache_96gb', 'C:/huggingface_cache_96gb']\n",
    "    for dir_path in cache_dirs:\n",
    "        os.makedirs(dir_path, exist_ok=True)\n",
    "        os.environ['TFHUB_CACHE_DIR'] = dir_path\n",
    "    \n",
    "    print(f\"üéØ Threads aumentados para: 48\")\n",
    "    print(f\"üíæ Cache 96GB configurado\")\n",
    "    print(f\"üöÄ PRONTO PARA DATASETS GIGANTES!\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "# ================= FUN√á√ÉO PARA TESTE DE PERFORMANCE (usar depois) =================\n",
    "def teste_performance_96gb(size=2000):\n",
    "    \"\"\"Teste de performance - USAR DEPOIS que kernel est√° est√°vel\"\"\"\n",
    "    if tf is None:\n",
    "        print(\"‚ùå TensorFlow n√£o carregado\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nüß™ Teste performance matriz {size}x{size}...\")\n",
    "    try:\n",
    "        import time\n",
    "        a = tf.random.normal((size, size), dtype=tf.float32)\n",
    "        b = tf.random.normal((size, size), dtype=tf.float32)\n",
    "        \n",
    "        start = time.perf_counter()\n",
    "        c = tf.linalg.matmul(a, b)\n",
    "        _ = c.numpy()\n",
    "        elapsed = time.perf_counter() - start\n",
    "        \n",
    "        gflops = (2 * size**3) / (elapsed * 1e9)\n",
    "        print(f\"‚úÖ {size}x{size}: {elapsed:.2f}s ({gflops:.1f} GFLOPs)\")\n",
    "        print(f\"üíæ Mem√≥ria usada: ~{(size*size*4*2)/1024**2:.0f}MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è  Teste falhou: {e}\")\n",
    "\n",
    "# ================= RECOMENDA√á√ïES DE USO =================\n",
    "print(f\"\\n\" + \"üéØ\" * 40)\n",
    "print(\"üéØ COMO USAR ESTE AMBIENTE:\")\n",
    "print(\"üéØ\" * 40)\n",
    "print(f\"1. Esta c√©lula j√° carregou TensorFlow com configura√ß√£o SEGURA\")\n",
    "print(f\"2. Para ATIVAR MODO 96GB COMPLETO, execute:\")\n",
    "print(f\"   ativar_modo_beast()\")\n",
    "print(f\"3. Para TESTAR PERFORMANCE, execute:\")\n",
    "print(f\"   teste_performance_96gb(5000)  # 5Kx5K matriz\")\n",
    "print(f\"4. Agora importe outras bibliotecas normalmente\")\n",
    "print(f\"\\n‚úÖ AMBIENTE INICIALIZADO COM SUCESSO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d23b55d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Carregando bibliotecas completas...\n",
      "‚úÖ Pandas 2.2.1\n",
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_estimator\\python\\estimator\\util.py:74: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\native_module.py:92: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Project_1\\ml_ai\\Lib\\site-packages\\tensorflow_hub\\saved_model_module.py:40: The name tf.saved_model.constants.LEGACY_INIT_OP_KEY is deprecated. Please use tf.compat.v1.saved_model.constants.LEGACY_INIT_OP_KEY instead.\n",
      "\n",
      "‚úÖ TensorFlow Hub 0.15.0\n",
      "‚úÖ Scikit-learn 1.4.2\n",
      "üéØ Todas bibliotecas carregadas!\n",
      "üí° Dica: Agora voc√™ pode usar ativar_modo_beast()\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üì¶ IMPORTS COMPLETOS - AGORA SEGURO\n",
    "# ============================================================================\n",
    "\n",
    "print(\"üì¶ Carregando bibliotecas completas...\")\n",
    "\n",
    "# Data Science b√°sico\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "print(f\"‚úÖ Pandas {pd.__version__}\")\n",
    "\n",
    "# TensorFlow Hub (se necess√°rio)\n",
    "try:\n",
    "    import tensorflow_hub as hub\n",
    "    print(f\"‚úÖ TensorFlow Hub {hub.__version__}\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  TensorFlow Hub n√£o dispon√≠vel\")\n",
    "\n",
    "# Scikit-learn\n",
    "import sklearn\n",
    "print(f\"‚úÖ Scikit-learn {sklearn.__version__}\")\n",
    "\n",
    "# Configura√ß√µes visuais\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"üéØ Todas bibliotecas carregadas!\")\n",
    "print(\"üí° Dica: Agora voc√™ pode usar ativar_modo_beast()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49e686aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö° Ativando pot√™ncia total 96GB...\n",
      "‚úÖ Threads aumentados para 32\n",
      "\n",
      "üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•\n",
      "üî• ATIVANDO MODO BEAST 96GB!\n",
      "üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•üî•\n",
      "üéØ Threads aumentados para: 48\n",
      "üíæ Cache 96GB configurado\n",
      "üöÄ PRONTO PARA DATASETS GIGANTES!\n",
      "\n",
      "üß™ Teste performance matriz 3000x3000...\n",
      "‚úÖ 3000x3000: 0.05s (1120.0 GFLOPs)\n",
      "üíæ Mem√≥ria usada: ~69MB\n",
      "\n",
      "üöÄ MODO 96GB ATIVADO! PRONTO PARA:\n",
      "   ‚Ä¢ Batch sizes at√© 4096\n",
      "   ‚Ä¢ Datasets de 50GB+ em RAM\n",
      "   ‚Ä¢ M√∫ltiplos modelos simult√¢neos\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üî• ATIVA√á√ÉO DO MODO 96GB - AP√ìS TUDO EST√ÅVEL\n",
    "# ============================================================================\n",
    "\n",
    "print(\"‚ö° Ativando pot√™ncia total 96GB...\")\n",
    "\n",
    "# 1. Primeiro aumente threads gradualmente\n",
    "os.environ['OMP_NUM_THREADS'] = '32'\n",
    "os.environ['MKL_NUM_THREADS'] = '32'\n",
    "print(f\"‚úÖ Threads aumentados para 32\")\n",
    "\n",
    "# 2. Importe bibliotecas pesadas\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# 3. Ative modo beast completo\n",
    "ativar_modo_beast()\n",
    "\n",
    "# 4. Teste performance\n",
    "teste_performance_96gb(3000)  # Comece com 3Kx3K\n",
    "\n",
    "print(\"\\nüöÄ MODO 96GB ATIVADO! PRONTO PARA:\")\n",
    "print(\"   ‚Ä¢ Batch sizes at√© 4096\")\n",
    "print(\"   ‚Ä¢ Datasets de 50GB+ em RAM\")\n",
    "print(\"   ‚Ä¢ M√∫ltiplos modelos simult√¢neos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5487fd7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ STATUS MEM√ìRIA:\n",
      "   ‚Ä¢ Total: 92GB\n",
      "   ‚Ä¢ Dispon√≠vel: 73GB\n",
      "   ‚Ä¢ Usado: 19GB\n",
      "   ‚Ä¢ Percentual: 20.5%\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# üõ†Ô∏è FUN√á√ïES ESPECIAIS PARA 96GB RAM\n",
    "# ============================================================================\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "def create_huge_dataset(dataset_size_gb=20):\n",
    "    \"\"\"Cria dataset gigante em RAM (s√≥ com 96GB!)\"\"\"\n",
    "    elements = int((dataset_size_gb * 1024**3) / 4 / 784)  # Para MNIST-like\n",
    "    x = tf.random.normal([elements, 28, 28, 1], dtype=tf.float32)\n",
    "    y = tf.random.uniform([elements], maxval=10, dtype=tf.int32)\n",
    "    print(f\"‚úÖ Dataset de {dataset_size_gb}GB criado: {x.shape}\")\n",
    "    return tf.data.Dataset.from_tensor_slices((x, y)).batch(1024)\n",
    "\n",
    "def memory_status():\n",
    "    \"\"\"Status detalhado da mem√≥ria\"\"\"\n",
    "    mem = psutil.virtual_memory()\n",
    "    print(f\"üíæ STATUS MEM√ìRIA:\")\n",
    "    print(f\"   ‚Ä¢ Total: {mem.total / 1024**3:.0f}GB\")\n",
    "    print(f\"   ‚Ä¢ Dispon√≠vel: {mem.available / 1024**3:.0f}GB\")\n",
    "    print(f\"   ‚Ä¢ Usado: {mem.used / 1024**3:.0f}GB\")\n",
    "    print(f\"   ‚Ä¢ Percentual: {mem.percent}%\")\n",
    "\n",
    "def train_large_batch(model, dataset, batch_size=2048):\n",
    "    \"\"\"Treina com batch_size gigante\"\"\"\n",
    "    print(f\"üöÄ Treinando com batch_size={batch_size} (96GB power!)\")\n",
    "    # Sua l√≥gica de treino aqui\n",
    "    pass\n",
    "\n",
    "# Verifica√ß√£o\n",
    "memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "881fef9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.15.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe23c8",
   "metadata": {},
   "source": [
    "# Download and becoming one with the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7becbc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get the data (10% of 10 food class from food101 dataset)\n",
    "# # https://www.kaggle.com/datasets/dansbecker/food-101\n",
    "\n",
    "# #import zipfile\n",
    "# #import urllib.request\n",
    "\n",
    "# #Baixar o arquivo ZIP\n",
    "# url = \"https://storage.googleapis.com/ztm_tf_course/food_vision/10_food_classes_10_percent.zip\"\n",
    "# zip_path = \"food_classes_10_percent.zip\"\n",
    "# urllib.request.urlretrieve(url, zip_path)\n",
    "\n",
    "# # # Descompactar o arquivo ZIP\n",
    "# with zipfile.ZipFile(zip_path, \"r\") as zip_ref:\n",
    "#     zip_ref.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2acaf697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 directories and 0 images in '10_food_classes_10_percent'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent\\test'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\chicken_curry'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\chicken_wings'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\fried_rice'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\grilled_salmon'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\hamburger'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\ice_cream'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\pizza'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\ramen'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\steak'.\n",
      "There are 0 directories and 250 images in '10_food_classes_10_percent\\test\\sushi'.\n",
      "There are 10 directories and 0 images in '10_food_classes_10_percent\\train'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\chicken_curry'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\chicken_wings'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\fried_rice'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\grilled_salmon'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\hamburger'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\ice_cream'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\pizza'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\ramen'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\steak'.\n",
      "There are 0 directories and 75 images in '10_food_classes_10_percent\\train\\sushi'.\n"
     ]
    }
   ],
   "source": [
    "# How many images in each folder?\n",
    "import os\n",
    "\n",
    "#WEalk through 10 percent data directory and list number of file\n",
    "for dirpath, dirnames, filenames in os.walk(\"10_food_classes_10_percent\"):\n",
    "    print(f\"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2033f0",
   "metadata": {},
   "source": [
    "## Create data loaders (preparing the data)\n",
    "\n",
    "We'll use the `ImageDataGenerator` class to load in our images in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d2aa3c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training images:\n",
      "Found 750 images belonging to 10 classes.\n",
      "Testing images:\n",
      "Found 2500 images belonging to 10 classes.\n"
     ]
    }
   ],
   "source": [
    "# Setup data inputs\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dir = \"10_food_classes_10_percent/train/\"\n",
    "test_dir = \"10_food_classes_10_percent/test/\"\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255.)\n",
    "\n",
    "print(\"Training images:\")\n",
    "train_data_10_percent = train_datagen.flow_from_directory(directory=train_dir,\n",
    "                                                           target_size=IMAGE_SIZE,\n",
    "                                                           batch_size=BATCH_SIZE,\n",
    "                                                           class_mode=\"categorical\")\n",
    "print(\"Testing images:\")\n",
    "test_data = test_datagen.flow_from_directory(directory=test_dir,\n",
    "                                                            target_size=IMAGE_SIZE,\n",
    "                                                            batch_size=BATCH_SIZE,\n",
    "                                                            class_mode=\"categorical\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f8d4e0",
   "metadata": {},
   "source": [
    "## Setting up callbacks (things to run whilst our model trains)\n",
    "\n",
    "Callbacks are extra functionality you can add to your models to be performed during or after training. Some of the most popular callbacks:\n",
    "\n",
    "* Tracking experiments with Tensorboard callback\n",
    "* Model checkpoint with the ModelCheckpoint callback\n",
    "* Stopping a model from training (before it trains too long and overfits) with the EarlyStopping callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "285a2a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TensorBoard callback (functionize because we need to create a new one for each model)\n",
    "import datetime # to help create unique log directory names\n",
    "def create_tensorboard_callback(dir_name, experiment_name):\n",
    "    log_dir = dir_name + \"/\" + experiment_name + \"/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") # create log directory\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir) # create TensorBoard callback\n",
    "    print(f\"Saving TensorBoard log files to: {log_dir}\")\n",
    "    return tensorboard_callback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a3ec70",
   "metadata": {},
   "source": [
    "> Note: You can customize the directory where your TensorBoared logs (model training metrics) get saved to whatever you like.\n",
    "The `log_dir` parameter we've created above is only one option.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab58374e",
   "metadata": {},
   "source": [
    "## Creating models using TensorFlow Hub\n",
    "\n",
    "In the past we've used TensorFlow to create our own models layers by layer from scratch.\n",
    "Now we're going to do a similar process, except the majority of our model's layers are going to come from TensorFlow Hub.\n",
    "We can access pretrained models on: https://tfhub.dev/\n",
    "\n",
    "Browsing the tensdorflow Hub page and sorting for image classification, we found the following feature vector model link: https://www.kaggle.com/models/google/efficientnet-v2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1f047a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classifica√ß√£o ImageNet\n",
    "resnet_v2_url = \"https://tfhub.dev/tensorflow/resnet_50/classification/1\"\n",
    "\n",
    "# Ou para recursos (feature vectors):\n",
    "resnet_feature_url = \"https://tfhub.dev/tensorflow/resnet_50/feature_vector/1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f8d535ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's make a create_model() function to reuse for each model\n",
    "\n",
    "from os import name\n",
    "\n",
    "\n",
    "def create_model(model_url, num_classes=10):\n",
    "    \"\"\"\n",
    "    Takes a TensorFlow Hub URL and creates a Keras Sequential model with it.\n",
    "    Args:\n",
    "      model_url (str): A TensorFlow Hub feature extraction URL.\n",
    "      num_classes (int): Number of output neurons/classes in the output layer, should be equal to Number of target classes,\n",
    "        default 10.\n",
    "\n",
    "    Returns:\n",
    "      An uncompiled Keras Sequential model with model_url as feature extractor layer and Dense output layer  with num_classes output neurons. \n",
    "    \"\"\"\n",
    "\n",
    "    # Download the pretrained model and save it as a Keras layer\n",
    "    feature_extractor_layer = hub.KerasLayer(model_url,\n",
    "                                             name=f\"feature_extractor_layer\", \n",
    "                                             input_shape=IMAGE_SIZE+(3,), # add 3 color channels to the image size\n",
    "                                             trainable=False) # freeze the already learned patterns\n",
    "\n",
    "    # Create our own model\n",
    "    model = tf.keras.Sequential([\n",
    "            feature_extractor_layer,\n",
    "            layers.Dense(num_classes, activation=\"softmax\", dtype='float32', name=\"output_layer\") # ensure output is float32\n",
    "        ])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bb2196",
   "metadata": {},
   "source": [
    "### Creating and testing ResNet TensorFlow Hub Feature Estraction model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dc55ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Resnet model\n",
    "resnet_model = create_model(resnet_feature_url,\n",
    "                             num_classes=train_data_10_percent.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3b649e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile our resnet model\n",
    "resnet_model.compile(loss=\"categorical_crossentropy\",\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "af6e0c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving TensorBoard log files to: tensorflow_hub/resnet50V2_feature_extraction/20260117-173350\n",
      "Epoch 1/5\n",
      "24/24 [==============================] - 42s 2s/step - loss: 0.7634 - accuracy: 0.7960 - val_loss: 0.7248 - val_accuracy: 0.7664\n",
      "Epoch 2/5\n",
      "24/24 [==============================] - 42s 2s/step - loss: 0.5417 - accuracy: 0.8533 - val_loss: 0.6255 - val_accuracy: 0.8056\n",
      "Epoch 3/5\n",
      "24/24 [==============================] - 43s 2s/step - loss: 0.3993 - accuracy: 0.9027 - val_loss: 0.5828 - val_accuracy: 0.8204\n",
      "Epoch 4/5\n",
      "24/24 [==============================] - 43s 2s/step - loss: 0.3180 - accuracy: 0.9240 - val_loss: 0.5663 - val_accuracy: 0.8208\n",
      "Epoch 5/5\n",
      "24/24 [==============================] - 44s 2s/step - loss: 0.2629 - accuracy: 0.9547 - val_loss: 0.5529 - val_accuracy: 0.8204\n"
     ]
    }
   ],
   "source": [
    "# Let's fit our resNet model to the data (10 percent of the food101 dataset)\n",
    "resnet_history = resnet_model.fit(train_data_10_percent,\n",
    "                                  epochs=5,\n",
    "                                  steps_per_epoch=len(train_data_10_percent),\n",
    "                                  validation_data=test_data,\n",
    "                                  validation_steps=len(test_data),\n",
    "                                  callbacks=[create_tensorboard_callback(dir_name=\"tensorflow_hub\",\n",
    "                                                                         experiment_name=\"resnet50V2_feature_extraction\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "153f5d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " feature_extractor_layer (K  (None, 2048)              23561152  \n",
      " erasLayer)                                                      \n",
      "                                                                 \n",
      " output_layer (Dense)        (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23581642 (89.96 MB)\n",
      "Trainable params: 20490 (80.04 KB)\n",
      "Non-trainable params: 23561152 (89.88 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "resnet_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16fe666",
   "metadata": {},
   "source": [
    "That is Increadible!!!. Our transfer learning feature extractor model out performed ALL of the previous models we built by hand... (substantially) and quicker training time AND with only 10% of the training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428892d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a function to plot loss curves\n",
    "#Tidbit: you could put this in a separate helper.py file and import it wherever you need it\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_loss_curves(history):\n",
    "    \"\"\"\n",
    "    Returns separate loss curves for training and validation metrics.\n",
    "    Args:\n",
    "      history: TensorFlow model History object (returned from model.fit()).\n",
    "    Returns:\n",
    "      Plots of training/validation loss and accuracy curves.\n",
    "    \"\"\"\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "\n",
    "    accuracy = history.history['accuracy']\n",
    "    val_accuracy = history.history['val_accuracy']\n",
    "\n",
    "    epochs = range(len(history.history['loss']))\n",
    "\n",
    "    # Plot loss\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, loss, label='Training Loss')\n",
    "    plt.plot(epochs, val_loss, label='Validation Loss')\n",
    "    plt.title('Loss Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, accuracy, label='Training Accuracy')\n",
    "    plt.plot(epochs, val_accuracy, label='Validation Accuracy')\n",
    "    plt.title('Accuracy Curves')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
